<!doctype html>
<html lang="en">
    <head>
        <title>Thinking in Dynamics: How Multimodal Large Language Models Perceive, Track, and Reason Dynamics in Physical 4D World</title>
        <link rel="icon" type="image/x-icon" href="./static/img/icons/brain.ico">

        <meta charset="utf-8">
        <meta name="viewport" content="width=device-width, initial-scale=1">

        <meta property="og:url" content="https://dxuan77.github.io/Thinking-in-Dynamics/" />
        <meta property="og:image" content="./static/img/preview.png" />
        <meta property="og:title" content="Thinking in Dynamics: How Multimodal Large Language Models Perceive, Track, and Reason Dynamics in Physical 4D World" />
        <meta property="og:description" content="Can MLLMs think in dynamics? We introduce Dyn-Bench to assess localized dynamics perception." />

        <meta name="twitter:url" content="https://dxuan77.github.io/Thinking-in-Dynamics/" />
        <meta name="twitter:card" content="summary_large_image" />
        <meta name="twitter:image" content="./static/img/preview.png" />
        <meta name="twitter:title" content="Thinking in Dynamics" />
        <meta name="twitter:description" content="Can MLLMs think in dynamics? We introduce Dyn-Bench." />

        <script src="./static/js/distill_template.v2.js"></script>
        <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
        <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>

        <script src="https://d3js.org/d3.v5.min.js"></script>
        <script src="https://d3js.org/d3-collection.v1.min.js"></script>
        <script src="https://rawgit.com/nstrayer/slid3r/master/dist/slid3r.js"></script>

        <script defer="" src="./static/js/hider.js"></script>
        <script src="./static/js/image_interact.js"></script>
        <script src="./static/js/switch_videos.js"></script>

        <link rel="stylesheet" href="./static/css/style.css">
        <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
        <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">

        <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.10.2/dist/katex.min.css" integrity="sha384-yFRtMMDnQtDRO8rLpMIKrtPCD5jdktao2TV19YiZYWMDkUR5GQZR/NOVTdquEx1j" crossorigin="anonymous">
        <script defer src="https://cdn.jsdelivr.net/npm/katex@0.10.2/dist/katex.min.js" integrity="sha384-9Nhn55MVVN0/4OFx7EE5kpFBPsEMZxKTCnA+4fqDmg12eCTqGi6+BB2LjY8brQxJ" crossorigin="anonymous"></script>
        <script defer src="https://cdn.jsdelivr.net/npm/katex@0.10.2/dist/contrib/auto-render.min.js" integrity="sha384-kWPLUVMOks5AQFrykwIup5lo0m3iMkkHrD0uJ4H5cjeGihAutqP0yW0J6dpFiVkI" crossorigin="anonymous" onload="renderMathInElement(document.body);"></script>
        <script defer src="./static/js/fontawesome.all.min.js"></script>

        <script src="https://cdn.jsdelivr.net/npm/jquery@3.7.1/dist/jquery.min.js"></script>
        <script defer src="./static/js/medium-zoom.min.js"></script>
        <script defer src="./static/js/zoom.js"></script>

        <style>
            /* 补充一些针对 Leaderboard 表格的样式，使其在目标模版中更好看 */
            .leaderboard-table {
                width: 100%;
                border-collapse: collapse;
                font-family: 'Inter', sans-serif;
                font-size: 0.85em;
                margin-top: 20px;
                box-shadow: 0 4px 20px rgba(0,0,0,0.05);
                border-radius: 8px;
                overflow: hidden;
            }
            .leaderboard-table th, .leaderboard-table td {
                padding: 10px 8px;
                text-align: center;
                border-bottom: 1px solid #eee;
            }
            .leaderboard-table th {
                background-color: #f8f9fa;
                font-weight: 600;
                cursor: pointer;
            }
            .leaderboard-table tr:hover {
                background-color: #f1f1f1;
            }
            .section-divider td {
                background-color: #eef2f5;
                font-weight: bold;
                text-align: left;
                padding-left: 20px;
            }
            .best-score {
                font-weight: bold;
                color: #d9534f;
            }
            .model-badge {
                font-size: 0.7em;
                padding: 2px 6px;
                border-radius: 4px;
                margin-left: 5px;
            }
            .open-badge { background-color: #d4edda; color: #155724; }
            .proprietary-badge { background-color: #f8d7da; color: #721c24; }
            
            /* 作者栏样式适配 */
            .author-block {
                text-align: center;
                margin-bottom: 30px;
            }
            .author-name {
                display: inline-block;
                margin: 0 10px;
                font-size: 1.1em;
                color: #0066cc;
            }
            .affiliation {
                display: block;
                margin-top: 10px;
                color: #555;
                font-size: 0.9em;
            }
        </style>
    </head>

    <body>
        <div class="header-wrapper">
            <div class="header-container" id="header-container">
                <div class="header-content">
                    <h1 style="margin-top: 0px">
                        <span style="font-size: 1.8em; display: block; margin-bottom: 0.3em; font-style: italic; font-family: 'Times New Roman', serif;">Thinking in Dynamics</span>
                        <span style="font-size: 1.0em; display: block; color: #FFF7D4; font-family: 'Times New Roman', serif;">How Multimodal Large Language Models Perceive, Track, and Reason Dynamics in Physical 4D World</span>
                    </h1>
                    
                    <div class="button-container">
                        <a href="TODO_ARXIV_ABS_URL" class="button paper-link" target="_blank">
                            <span class="icon is-small">
                                <i class="ai ai-arxiv" style="height: 1.5em;"></i>
                            </span>
                            ArXiv
                        </a>
                        <a href="TODO_PDF_URL" class="button paper-link" target="_blank">
                            <span class="icon is-small" style="height: 1.5em;">
                                <i class="fas fa-file-pdf"></i>
                            </span>
                            <span>PDF</span>
                        </a>
                        <a href="TODO_CODE_REPO_URL" class="button" target="_blank">
                            <span class="icon is-small" style="height: 1.5em;">
                                <i class="fab fa-github"></i>
                            </span>
                            <span>Code</span>
                        </a>
                        <a href="TODO_DATASET_URL" class="button" target="_blank">
                            <span class="icon is-small">
                                <img src="https://huggingface.co/front/assets/huggingface_logo-noborder.svg" alt="Hugging Face logo" style="height: 1.5em;">
                            </span>
                            <span>Dyn-Bench</span>
                        </a> 
                        <a href="#dyn-leaderboard" class="button">
                            <span class="icon is-small">
                                <i class="fas fa-trophy"></i>
                            </span>
                            <span>Leaderboard</span>
                        </a>
                    </div>
                </div>
                <div class="header-image">
                    <img draggable="false" src="static/img/preview.png" alt="Teaser Image" class="teaser-image">
                </div>
            </div>
        </div>


        <d-article>
            
            <div class="author-block">
                <div class="authors">
                    <a href="https://github.com/Dxuan77" class="author-name">Dongxuan Liu<sup>△*</sup></a>
                    <a href="https://github.com/vealocia" class="author-name">Shusheng Yang<sup>△*</sup></a>
                    <a href="https://anjaliwgupta.com" class="author-name">Anjali W. Gupta<sup>△*</sup></a>
                    <a href="https://rilynhan.github.io" class="author-name">Rilyn Han<sup>▲*</sup></a>
                    <a href="https://profiles.stanford.edu/fei-fei-li" class="author-name">Li Fei-Fei<sup>★</sup></a>
                    <a href="https://www.sainingxie.com/" class="author-name">Saining Xie<sup>△</sup></a>
                </div>
                <div class="affiliations affiliation">
                    <span><sup>△</sup>Xiamen University</span> &nbsp;&nbsp;
                    <span><sup>▲</sup>Yale University</span> &nbsp;&nbsp;
                    <span><sup>★</sup>Stanford University</span>
                </div>
                <div style="margin-top: 10px; font-size: 0.8em; color: gray;">*Equal contribution</div>
            </div>

            <div id='teaser' class="sub-section">
                <d-figure id="fig-teaser" >
                    <figure>
                        <img data-zoomable="" draggable="false" src="static/img/teaser.png" alt="Dyn-Bench Framework">
                        <figcaption>
                            <strong>Figure 1: Spatio-temporal dynamics reasoning benchmark Dyn-Bench.</strong> It rigorously evaluates multimodal large language models on their ability to perceive, track, and reason about dynamic contents in the 4D world.
                        </figcaption>
                    </figure>
                </d-figure>
            </div>
            
            <div style="width: 150%; max-width: 2400px; margin: 0 auto; text-align: left; margin-left: -18%;">
                <p>
                    <strong>TLDR:</strong> Can MLLMs think in dynamics? We introduce <strong>Dyn-Bench</strong> to systematically assess the spatio-temporal reasoning and localized dynamics perception of MLLMs in the physical 4D world.
                </p>
                <ul style="display: inline-block; text-align: left;">
                    <li>We evaluate general, spatial, and region-level MLLMs, finding that existing models cannot simultaneously maintain strong performance in both spatio-temporal reasoning and dynamic object grounding.</li>
                    <li>We find that structured integration approaches, including <strong>Mask-Guided Fusion</strong> and <strong>Spatio-Temporal Textual Cognitive Map (ST-TCM)</strong>, significantly enhance MLLMs’ dynamics perception and reasoning.</li>
                    <li>Dyn-Bench comprises 1k videos, 7k visual question answering (VQA) pairs, and 3k dynamic object grounding pairs, collected from diverse real-world and synthetic datasets.</li>
                </ul>
            </div>


        <div class="vision-block">  
        <div class="icon-row">
            <a href="#dyn-benchmark" class="icon-link">
                <img src="./static/img/icons/bench.svg" alt="Bench Icon" class="icon">
                Dyn-Bench<br>Construction
            </a>
            <a href="#dyn-evaluation" class="icon-link">
                <img src="./static/img/icons/evaluation.svg" alt="Eval Icon" class="icon">
                Evaluation<br>Protocol
            </a>
            <a href="#dyn-leaderboard" class="icon-link">
                <img src="./static/img/leaderboard-star-svgrepo-com.svg" alt="Leaderboard Icon" class="icon">
                Dyn-Bench<br>Leaderboard
            </a>
            <a href="#visual-representation" class="icon-link">
                <img src="./static/img/icons/speech.svg" alt="Text Icon" class="icon">
                Textual<br>Guidance
            </a>
            <a href="#vsi-improvements" class="icon-link">
                <img src="./static/img/icons/vision.svg" alt="Vision Icon" class="icon">
                Visual<br>Guidance
            </a>
        </div>

        <p class="click-hint" style="width: 85%;">
            <img src="static/img/icons/click.gif" style="width: 1.5rem">
            <strong>Click to jump to each section.</strong>
        </p>
         <hr style="width: 50%;">

        <div id='dyn-benchmark' class="sub-section">
            <h2 class="text">Dyn-Bench Construction</h2>
            <p class="text">
                We construct Dyn-Bench by collecting dynamic videos from four 2D video segmentation datasets and four 4D dynamic-scene datasets. These datasets provide instance masks, depth maps, and camera poses, enabling accurate question-answer generation and object category annotation.
            </p>
            <d-figure id="fig-dataengine" >
                <figure>
                    <img data-zoomable="" draggable="false" src="static/img/dataengine_v8.png" alt="Benchmark Curation Pipeline">
                    <figcaption><strong>Figure 2: Benchmark curation pipeline.</strong> Integration, Multimodal Completion, Filtering, and Verification.</figcaption>
                </figure>
            </d-figure>
            <br>
            <d-figure id="fig-stats" >
                <figure>
                    <img data-zoomable="" draggable="false" src="static/img/data_statistics_v5.png" alt="Benchmark Statistics" style="width: 60%; margin: 0 auto; display: block;">
                    <figcaption><strong>Figure 3: Benchmark Statistics.</strong> Task distribution and VQA pairs.</figcaption>
                </figure>
            </d-figure>
        </div>

        <div class="vision-block">
            <div id="dyn-evaluation" class="sub-section">
            <h1 class="text">Evaluation on Dyn-Bench</h1>
                <p class="text">
                    Proprietary general models (GPT-4o) remain strong on spatio-temporal reasoning, especially inter-object dynamics. Region-level models (e.g., Sa2VA, UniPixel) lead in object-centric reasoning and grounding.
                </p>
                <d-figure id="fig-radar" >
                    <figure>
                        <img data-zoomable="" draggable="false" src="static/img/radar_v10-1.png" alt="Radar Analysis" style="width: 80%; display: block; margin: 0 auto;">
                         <figcaption><strong>Figure 5: Model performance on Dyn-Bench.</strong> Radar charts for general, spatial, and region-level MLLMs.</figcaption>
                    </figure>
                </d-figure>
            </div>
        </div>

        <br>
        </br>

        <div class="vision-block">
            <div id="dyn-leaderboard" class="sub-section">
            <h1 class="text">Dyn-Bench Leaderboard</h1>
                <p class="text" style="text-align: center;">
                    To include your model, please email <a href="mailto:jihanyang13@gmail.com">jihanyang13@gmail.com</a>.
                </p>      
                
                <div class="leaderboard-container" style="overflow-x: auto;">
                    <table class="leaderboard-table">
                        <thead>
                            <tr class="header-group">
                                <th rowspan="2">Model</th>
                                <th rowspan="2">LLM Params</th>
                                <th rowspan="2">Avg.</th>
                                <th colspan="4">Numerical Answer Questions</th>
                                <th colspan="4">Multiple-Choice Questions</th>
                            </tr>
                            <tr>
                                <th>Obj. Count</th>
                                <th>Abs. Dist.</th>
                                <th>Obj. Size</th>
                                <th>Room Size</th>
                                <th>Rel. Dist.</th>
                                <th>Rel. Dir.</th>
                                <th>Route Plan</th>
                                <th>Appear. Order</th>
                            </tr>
                        </thead>
                        <tbody>
                            <tr class="section-divider"><td colspan="12">Baselines</td></tr>
                            <tr>
                                <td>Human-Level</td>
                                <td>-</td>
                                <td class="best-score">79.2</td>
                                <td class="best-score">94.3</td>
                                <td>47.0</td>
                                <td>60.4</td>
                                <td>45.9</td>
                                <td class="best-score">94.7</td>
                                <td class="best-score">95.8</td>
                                <td class="best-score">95.8</td>
                                <td class="best-score">100.0</td>
                            </tr>
                            <tr>
                                <td>Chance-level (Random)</td>
                                <td>-</td>
                                <td>-</td>
                                <td>-</td>
                                <td>-</td>
                                <td>-</td>
                                <td>-</td>
                                <td>25.0</td>
                                <td>36.1</td>
                                <td>28.3</td>
                                <td>25.0</td>
                            </tr>
                            
                            <tr class="section-divider"><td colspan="12">MLLMs</td></tr>
                            
                            <tr>
                                <td>SpaceMind <span class="model-badge open-badge">Open</span></td>
                                <td>7B</td>
                                <td>69.2</td>
                                <td>72.7</td>
                                <td>60.3</td>
                                <td>78.0</td>
                                <td>75.5</td>
                                <td>71.0</td>
                                <td>87.9</td>
                                <td>42.8</td>
                                <td>65.9</td>
                            </tr>
                            <tr>
                                <td>Gemini-1.5-Pro <span class="model-badge proprietary-badge">Proprietary</span></td>
                                <td>-</td>
                                <td>48.8</td>
                                <td>49.6</td>
                                <td>28.8</td>
                                <td>58.6</td>
                                <td>49.4</td>
                                <td>46.0</td>
                                <td>48.1</td>
                                <td>42.0</td>
                                <td>68.0</td>
                            </tr>
                            <tr>
                                <td>InternVL3-78B <span class="model-badge open-badge">Open</span></td>
                                <td>78B</td>
                                <td>48.4</td>
                                <td>71.2</td>
                                <td>53.7</td>
                                <td>44.4</td>
                                <td>39.5</td>
                                <td>55.9</td>
                                <td>39.5</td>
                                <td>28.9</td>
                                <td>54.5</td>
                            </tr>
                            <tr>
                                <td>GPT-4o <span class="model-badge proprietary-badge">Proprietary</span></td>
                                <td>-</td>
                                <td>47.8</td>
                                <td>43.1</td>
                                <td>34.1</td>
                                <td>68.6</td>
                                <td>64.2</td>
                                <td>48.3</td>
                                <td>43.1</td>
                                <td>29.4</td>
                                <td>51.3</td>
                            </tr>
                            </tbody>
                    </table>
                </div>
            </div>
        </div>

        <div class="vision-block">
            <div id="visual-representation" class="sub-section">
                <h1 class="text">Textual & Visual Guidance</h1>
                <p class="text">
                    <strong>Textual:</strong> We probe GPT-4o’s self-explanations. Without explicit cues, it often relies on surface descriptions. When provided a spatio-temporal textual map (ST-TCM), the model starts to anchor objects to coordinates and reason about relative velocities.
                </p>
                <d-figure id="fig-case-study" >
                    <figure>
                        <img data-zoomable="" draggable="false" src="static/img/faliurecase_v10-1.png" alt="Failure Case Study" style="width: 90%; display: block; margin: 0 auto; border-radius: 12px; box-shadow: 0 4px 20px rgba(0,0,0,0.2);">
                    </figure>
                </d-figure>
                
                <br><br>

                <div id="vsi-improvements">
                     <p class="text">
                        <strong>Visual:</strong> Mask-guided fusion yields the strongest grounding across all tasks, outperforming raw video and mask-only inputs on inter-object, object-scene, and camera-object reasoning.
                    </p>
                    <d-figure id="fig-vis-guide" >
                        <figure>
                            <img data-zoomable="" draggable="false" src="static/img/visual_v5-1.png" alt="Visual Guidance Comparison" style="width: 90%; display: block; margin: 0 auto;">
                        </figure>
                    </d-figure>
                </div>
            </div>
         </div>


        <div id="conclusion" style="position: relative; margin-top: 40px; margin-bottom: 0px;">
            <h1 class="text" style="margin-top:0px; margin-bottom:10px">Conclusion</h1>
            <p class="text">
                We study how MLLMs think in dynamics via Dyn-Bench. Spatio-Temporal Textual Cognitive Maps (ST-TCM) provide structured linguistic abstractions that strengthen temporal coherence and relational reasoning, while mask-guided visual grounding enhances motion perception. Reliable dynamic understanding emerges from coupling high-level temporal semantics with localized region-level grounding.
            </p>
        </div>

        <div id="bibtex" style="position: relative; margin-top: 40px; margin-bottom: 40px; color: gray;">
            <h2 class="text" style="margin-top:0px; margin-bottom:10px">BibTeX</h2>
            
            <pre style="white-space: pre-wrap; word-break: break-word; background-color: #f5f5f5; padding: 25px; border-radius: 12px; font-family: monospace; font-size: 0.9em; text-align: left; overflow-x: hidden; width: 200%; position: relative; left: 50%; transform: translateX(-50%); box-sizing: border-box; box-shadow: 0 4px 20px rgba(0,0,0,0.05);">
@article{TODO_dynamics,
    title={{Thinking in Dynamics: How Multimodal Large Language Models Perceive, Track, and Reason Dynamics in Physical 4D World}},
    author={Dongxuan Liu and Shusheng Yang and Anjali W. Gupta and Rilyn Han and Li Fei-Fei and Saining Xie},
    year={2024},
    journal={arXiv preprint},
}</pre>
        </div>
        
        <script src="./static/js/nav-bar.js"></script>
        <script>
            document.querySelectorAll('.leaderboard-table th').forEach((header, index) => {
                header.addEventListener('click', () => {
                    const table = header.parentElement.parentElement.parentElement;
                    const rows = Array.from(table.querySelectorAll('tr:not(.header-group):not(.section-divider)'));
                    const isAsc = header.classList.contains('asc');
                    
                    rows.sort((a, b) => {
                        const cellA = a.children[index].innerText;
                        const cellB = b.children[index].innerText;
                        return isAsc ? cellA.localeCompare(cellB, undefined, {numeric: true}) : cellB.localeCompare(cellA, undefined, {numeric: true});
                    });

                    rows.forEach(row => table.querySelector('tbody').appendChild(row));
                    header.classList.toggle('asc');
                });
            });
        </script>
    </body>
</html>